# <p align="center"> <b>ğŸ’» LLM Evaluation Dashboard ğŸ“ˆ</b> </p>

<p align="center"> <img width="1500" img height="500" src="https://github.com/SouravD-Me/LLM-Evaluation-Dashboard/blob/main/LLM%20Evaluation%20Dashboard.png"> </p> 
<p align="center"> <em>ğŸŒŸ A Dash-powered interactive dashboard for benchmarking and visualizing the performance of large language models across several datasets. ğŸŒŸ</em> </p> 
<p align="center"> <a href="https://github.com/Sourav-Das1996/llm_evaluation_dashboard/blob/main/LICENSE"> <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="MIT License"> </a> 
    <a href="https://github.com/Sourav-Das1996/llm_evaluation_dashboard/pulls"> <img src="https://img.shields.io/github/issues-pr/Sourav-Das1996/llm_evaluation_dashboard.svg" alt="Pull Requests"> </a> 
    <a href="https://github.com/Sourav-Das1996/llm_evaluation_dashboard/stargazers"> <img src="https://img.shields.io/github/stars/Sourav-Das1996/llm_evaluation_dashboard.svg" alt="Stars"> </a> <a href="https://github.com/Sourav-Das1996/llm_evaluation_dashboard/network/members"> <img src="https://img.shields.io/github/forks/Sourav-Das1996/llm_evaluation_dashboard.svg" alt="Forks"> </a> </p> 
<p align="center"> <a href="#key-features">Key Features</a> â€¢ <a href="#demo">Demo</a> â€¢ <a href="#installation">Installation</a> â€¢ <a href="#usage">Usage</a> â€¢ <a href="#contributing">Contributing</a> â€¢ <a href="#license">License</a> </p>

## ğŸŸ¡ Key Features: âœ¨

    ğŸƒâ€â™‚ï¸ Multi-process benchmarking for faster evaluation of multiple models,
    ğŸ“Š Interactive visualizations for model performance metrics like generation time, token length, etc.,
    ğŸ”¢ Numerical summary statistics for each model,
    ğŸŒˆ Customizable selection of models, datasets, and token length range,
    ğŸ¨ Polished UI with a dark-themed Bootstrap style,
    ğŸŒ Cross-platform compatibility with Dash.

## ğŸŸ¡ Demo: ğŸ¥
<p align="center"> <img src="https://github.com/SouravD-Me/LLM-Evaluation-Dashboard/blob/main/LLM%20Evaluation%20-%20MiniDemo.gif" alt="Demo GIF"> </p>

## ğŸŸ¡ Installation: ğŸš€

### ğŸ”¶ Clone the repository:

```bash
git clone https://github.com/Sourav-Das1996/llm_evaluation_dashboard.git
```

### ğŸ”¶ Change to the project directory:

cd llm_evaluation_dashboard

### ğŸ”¶ Install the required packages:

pip install -r requirements.txt

## ğŸŸ¡ Usage: ğŸ•¹ï¸

### ğŸ”¶ Run the application:

python app.py (LLM Evaluation - Main.ipynb)

    1. Open your web browser and navigate to http://localhost:8050/ to access the dashboard.
    2. Customize the models, datasets, and token length range using the interactive controls.
    3. Explore the various visualizations to analyze the performance of the selected models.
    4. Download the benchmark results as a CSV file for further analysis.

## ğŸŸ¡ Contributing: ğŸ¤

### I welcome contributions from the community! If you'd like to contribute, please follow these steps:

    1. Fork the repository.
    2. Create a new branch for your feature or bug fix: git checkout -b my-new-feature.
    3. Make your changes and commit them: git commit -am 'Add new feature'.
    4. Push your changes to your branch: git push origin my-new-feature.
    5. Submit a pull request.

## ğŸŸ¡ License: âš–ï¸

This project is licensed under the MIT License. See the LICENSE file for more details.

### Made with ğŸ™ğŸ» by Sourav Das
