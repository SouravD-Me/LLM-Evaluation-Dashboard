# <p align="center"> <b>💻 LLM Evaluation Dashboard 📈</b> </p>

<p align="center"> <img width="1500" img height="500" src="https://github.com/SouravD-Me/LLM-Evaluation-Dashboard/blob/main/LLM%20Evaluation%20Dashboard.png"> </p> 
<p align="center"> <em>🌟 A Dash-powered interactive dashboard for benchmarking and visualizing the performance of large language models across several datasets. 🌟</em> </p> 
<p align="center"> <a href="https://github.com/Sourav-Das1996/llm_evaluation_dashboard/blob/main/LICENSE"> <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="MIT License"> </a> 
    <a href="https://github.com/Sourav-Das1996/llm_evaluation_dashboard/pulls"> <img src="https://img.shields.io/github/issues-pr/Sourav-Das1996/llm_evaluation_dashboard.svg" alt="Pull Requests"> </a> 
    <a href="https://github.com/Sourav-Das1996/llm_evaluation_dashboard/stargazers"> <img src="https://img.shields.io/github/stars/Sourav-Das1996/llm_evaluation_dashboard.svg" alt="Stars"> </a> <a href="https://github.com/Sourav-Das1996/llm_evaluation_dashboard/network/members"> <img src="https://img.shields.io/github/forks/Sourav-Das1996/llm_evaluation_dashboard.svg" alt="Forks"> </a> </p> 
<p align="center"> <a href="#key-features">Key Features</a> • <a href="#demo">Demo</a> • <a href="#installation">Installation</a> • <a href="#usage">Usage</a> • <a href="#contributing">Contributing</a> • <a href="#license">License</a> </p>

## 🟡 Key Features: ✨

    🏃‍♂️ Multi-process benchmarking for faster evaluation of multiple models,
    📊 Interactive visualizations for model performance metrics like generation time, token length, etc.,
    🔢 Numerical summary statistics for each model,
    🌈 Customizable selection of models, datasets, and token length range,
    🎨 Polished UI with a dark-themed Bootstrap style,
    🌐 Cross-platform compatibility with Dash.

## 🟡 Demo: 🎥
<p align="center"> <img src="https://github.com/SouravD-Me/LLM-Evaluation-Dashboard/blob/main/LLM%20Evaluation%20-%20MiniDemo.gif" alt="Demo GIF"> </p>

## 🟡 Installation: 🚀

### 🔶 Clone the repository:

```bash
git clone https://github.com/Sourav-Das1996/llm_evaluation_dashboard.git
```

### 🔶 Change to the project directory:

cd llm_evaluation_dashboard

### 🔶 Install the required packages:

pip install -r requirements.txt

## 🟡 Usage: 🕹️

### 🔶 Run the application:

python app.py (LLM Evaluation - Main.ipynb)

    1. Open your web browser and navigate to http://localhost:8050/ to access the dashboard.
    2. Customize the models, datasets, and token length range using the interactive controls.
    3. Explore the various visualizations to analyze the performance of the selected models.
    4. Download the benchmark results as a CSV file for further analysis.

## 🟡 Contributing: 🤝

### I welcome contributions from the community! If you'd like to contribute, please follow these steps:

    1. Fork the repository.
    2. Create a new branch for your feature or bug fix: git checkout -b my-new-feature.
    3. Make your changes and commit them: git commit -am 'Add new feature'.
    4. Push your changes to your branch: git push origin my-new-feature.
    5. Submit a pull request.

## 🟡 License: ⚖️

This project is licensed under the MIT License. See the LICENSE file for more details.

### Made with 🙏🏻 by Sourav Das
